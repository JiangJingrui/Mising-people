# Missing Persons Information Collection, Classification, and Extraction

This is a repository that contains the source code for the "Missing Persons Information Collection, Classification, and Extraction" project. In this work, we conducted extensive research on the collection and classification of missing persons information on the popular online social network Weibo in China.

## Code
In this repository, the source code of the project is released for replication and further use, including:

### Network crawling
Code for crawling Weibo information related to missing persons. The main spider class `SearchSpider` in `Weibo search master/web/spider/search.py` is responsible for building search URLs based on different search criteria (such as keywords, time range, region), and then parsing Weibo information on web pages, including user information, Weibo content, shares, comments, likes, etc.

Configure search criteria in the `settings.py` file, such as keywords, time range, region, etc.

### High frequency keyword extraction
The `extract_rashtags` function in `Mising people/Financial classification model/search_keywords_generator.py` can extract specific tags from Positive text, which can be used as search keywords to improve the accuracy of information collection.

### Data preprocessing
Convert text data into a format suitable for model training while preserving the features used.

### Binary Model training, testing
There are multiple machine learning models in the `model` directory under `Mising people/Binary classification`, including `TextCNN`, `Bert`, `SVM`, and `RF`, which can be used for binary classification of missing person related information to help identify whether Weibo is related to missing person events.

### Information extraction based on LLMs
We have implemented a set of code that utilizes Large Language Models (LLMs) to extract key information related to missing persons from Weibo posts. The scripts in the `LLMs` directory are designed to interact with pre-trained LLMs. Firstly, they develop specific prompts based on task requirements, such as extracting detailed information about missing persons (age, gender, appearance), the time and location of their disappearance, and any relevant contact information. After receiving the response from LLM, the code will perform data cleaning and validation. For example, it verifies whether the extracted time and location follow a valid format.

## Data
In terms of data, we have released training and testing datasets for machine learning models, which are stored as text files in the `data` directory under `Mising people/Financial classification model/TextCNN/Chinese text classification/`. The dataset includes Weibo text and corresponding labels, which can be used to evaluate binary classification models. We also released the dataset and standard set generated by LLMs for information extraction in the `LLMs/data` directory, which can be used for training and evaluating large language models.

## Model training and prediction
1. Select the appropriate model from the `model` directory.
2. Train the model using the training dataset and evaluate it using the testing dataset.
3. Use a trained model to predict new posts.

## Necessary condition
- Python 3.x
- Library for interacting with LLM
- Scrapy： Used for web crawling.
- PyTorch： Used for building and training machine learning models.
- Other necessary libraries such as `numpy`, `pickle`, `re`, etc. 
