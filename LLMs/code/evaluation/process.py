#Data preprocessing
import json

with open('/your Standard Dataset/path/', 'r', encoding='utf-8') as f1:
    train_data = json.load(f1)

with open('/your Dataset generated by large models/path/', 'r', encoding='utf-8') as f2:
    zero_shot_data = json.load(f2)

processed_data = []
for item in zero_shot_data:
    new_item = {
    "text": item["text"],
    "info": {
        "name": item.get("info", {}).get("name"),  
        "gender": item.get("info", {}).get("gender"),
        "missing_age": item.get("info", {}).get("missing_age"),
        "missing_height": item.get("info", {}).get("missing_height"),
        "missing_date": item.get("info", {}).get("missing_date"),  
        "missing_location": item.get("info", {}).get("missing_location")
    }
}
    processed_data.append(new_item)

with open('/processed dataset/path/', 'w', encoding='utf-8') as f:
    json.dump(processed_data, f, ensure_ascii=False, indent=4)
